{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97398a4-6443-47b8-8ac6-55e602817cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6f0e424-a598-4bcf-82b5-d620088bca01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "amazon = pd.read_csv(\"metadata.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "566c2371-6527-4edd-afe3-221fa43a8507",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Text_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>DB Longboards CoreFlex Crossbow 41\" Bamboo Fib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>Electronic Snap Circuits Mini Kits Classpack, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>3Doodler Create Flexy 3D Printing Filament Ref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>Guillow Airplane Design Studio with Travel Cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>Woodstock- Collage 500 pc Puzzle|Toys &amp; Games ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image  \\\n",
       "0  https://images-na.ssl-images-amazon.com/images...   \n",
       "1  https://images-na.ssl-images-amazon.com/images...   \n",
       "2  https://images-na.ssl-images-amazon.com/images...   \n",
       "3  https://images-na.ssl-images-amazon.com/images...   \n",
       "4  https://images-na.ssl-images-amazon.com/images...   \n",
       "\n",
       "                                    Text_Description  \n",
       "0  DB Longboards CoreFlex Crossbow 41\" Bamboo Fib...  \n",
       "1  Electronic Snap Circuits Mini Kits Classpack, ...  \n",
       "2  3Doodler Create Flexy 3D Printing Filament Ref...  \n",
       "3  Guillow Airplane Design Studio with Travel Cas...  \n",
       "4  Woodstock- Collage 500 pc Puzzle|Toys & Games ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63345eae-691f-4db3-b627-d1fed30a64f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1776, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3881ee1-345a-4815-9305-3399af31ea85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import getpass\n",
    "\n",
    "# # Prompt for OpenAI API key securely\n",
    "# api_key = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# # Set the API key as an environment variable\n",
    "# os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "624b77ff-e2af-4ca5-9303-3090271ec899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323b27e-ddd3-42ed-b6fb-7041c4a88ed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n",
    "# Load pretrained CLIP model and processor\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_name)\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Ensure the proper device is set (CUDA if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "clip_model.to(device)\n",
    "clip_model.eval()  # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13b62b18-817d-46ef-9814-7857097d301c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 1776/1776 [00:16<00:00, 108.86it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def truncate_text(text, max_length=77):\n",
    "    \"\"\"\n",
    "    Truncates a text to a specified maximum number of tokens.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The input text to be truncated.\n",
    "        max_length (int): Maximum number of tokens allowed.\n",
    "\n",
    "    Returns:\n",
    "        str: Truncated text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return \" \".join(words[:max_length])\n",
    "\n",
    "\n",
    "def get_text_embeddings_with_truncation(texts, max_length=77, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generates embeddings for text using truncation to fit the maximum token length.\n",
    "    \n",
    "    Parameters:\n",
    "        texts (List[str]): List of input texts.\n",
    "        max_length (int): Maximum length of tokens for CLIP (default is 77 tokens).\n",
    "        batch_size (int): Batch size for processing text.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list where each entry contains `truncated_text`, `embedding`, and `text_id`.\n",
    "    \"\"\"\n",
    "    all_text_embeddings = []\n",
    "    for text_id, text in enumerate(tqdm(texts, desc=\"Processing texts\")):\n",
    "        # Step 1: Truncate the text to the maximum allowed length\n",
    "        truncated_text = truncate_text(text, max_length=max_length)\n",
    "        \n",
    "        # Step 2: Process the truncated text in batches\n",
    "        inputs = clip_processor(\n",
    "            text=[truncated_text], return_tensors=\"pt\", padding=True, truncation=True\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = clip_model.get_text_features(**inputs)\n",
    "\n",
    "        # Step 3: Store the embedding with metadata\n",
    "        all_text_embeddings.append({\n",
    "            \"text_id\": text_id,\n",
    "            \"truncated_text\": truncated_text,\n",
    "            \"embedding\": text_features.cpu()\n",
    "        })\n",
    "\n",
    "    return all_text_embeddings\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text_descriptions = amazon[\"Text_Description\"].tolist()  # Replace with your DataFrame column\n",
    "text_embeddings = get_text_embeddings_with_truncation(\n",
    "    text_descriptions, max_length=77, batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19998730-1d61-468e-9fb8-7fe648003a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1776"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf4787b7-ce1e-476d-97c2-3011d1496041",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [03:08<00:00,  6.72s/it]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Generate image embeddings in batches\n",
    "def get_image_embeddings_in_batches(image_urls, batch_size=16):\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(image_urls), batch_size)):\n",
    "        batch_urls = image_urls[i:i + batch_size]\n",
    "        \n",
    "        # Fetch and preprocess the images in the batch\n",
    "        image_list = []\n",
    "        for url in batch_urls:\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "                image_list.append(image)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching or processing image from {url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not image_list:\n",
    "            continue\n",
    "        \n",
    "        inputs = clip_processor(images=image_list, return_tensors=\"pt\", padding=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Generate embeddings for the batch\n",
    "            image_embeddings = clip_model.get_image_features(**inputs)\n",
    "        \n",
    "        # Move to CPU and append to results\n",
    "        all_embeddings.append(image_embeddings.cpu())\n",
    "    \n",
    "    # Concatenate all batch embeddings\n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "# Example usage\n",
    "image_urls = amazon[\"Image\"].tolist()  # Replace `amazon` with your DataFrame variable\n",
    "image_embeddings = get_image_embeddings_in_batches(image_urls, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "294e8756-79cb-4094-9f34-076dc14ec02e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1776, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afbccf53-d8d4-41cf-8eb6-582fd75a9d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built with 3552 embeddings.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Initialize FAISS index\n",
    "embedding_dim = 512\n",
    "index = faiss.IndexFlatIP(embedding_dim)  # Use Inner Product for cosine similarity\n",
    "\n",
    "# Initialize metadata storage\n",
    "metadata_faiss = []\n",
    "\n",
    "# Normalize and add text embeddings\n",
    "for entry in text_embeddings:\n",
    "    embedding = entry[\"embedding\"].numpy()\n",
    "    norm = np.linalg.norm(embedding)\n",
    "    if norm > 0:\n",
    "        normalized_embedding = embedding / norm  # Normalize\n",
    "    else:\n",
    "        normalized_embedding = embedding  # Handle zero vector case\n",
    "\n",
    "    normalized_embedding = normalized_embedding.astype(np.float32)  # Ensure float32\n",
    "    index.add(normalized_embedding.reshape(1, -1))  # Add to FAISS\n",
    "    metadata_faiss.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text_id\": entry[\"text_id\"],\n",
    "        \"content\": entry[\"truncated_text\"]  # Store truncated text for reference\n",
    "    })\n",
    "\n",
    "# Normalize and add image embeddings\n",
    "for i, embedding in enumerate(image_embeddings):\n",
    "    embedding = embedding.numpy()\n",
    "    norm = np.linalg.norm(embedding)\n",
    "    if norm > 0:\n",
    "        normalized_embedding = embedding / norm  # Normalize\n",
    "    else:\n",
    "        normalized_embedding = embedding  # Handle zero vector case\n",
    "\n",
    "    normalized_embedding = normalized_embedding.astype(np.float32)  # Ensure float32\n",
    "    index.add(normalized_embedding.reshape(1, -1))  # Add to FAISS\n",
    "    metadata_faiss.append({\n",
    "        \"type\": \"image\",\n",
    "        \"image_id\": i,\n",
    "        \"path\": image_urls[i]  # Store the image file path or URL for reference\n",
    "    })\n",
    "\n",
    "# Ensure metadata and FAISS index are aligned\n",
    "assert len(metadata_faiss) == index.ntotal, \"Metadata and FAISS index embeddings count mismatch!\"\n",
    "\n",
    "print(f\"FAISS index built with {index.ntotal} embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36609a22-fa58-4d5f-938d-5b80be29d535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save FAISS index to a file\n",
    "faiss.write_index(index, \"faiss_index.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51d65cd3-cde8-40d4-bc34-5473f016e736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save metadata to a file\n",
    "with open(\"metadata_faiss.pkl\", \"wb\") as f:\n",
    "    pickle.dump(metadata_faiss, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c78fa815-ba7a-48e1-9000-4c0bd37fb687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load FAISS index from the file\n",
    "index = faiss.read_index(\"faiss_index.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "818f10c8-f827-4969-837e-e8c66d14a8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load metadata from the file\n",
    "with open(\"metadata_faiss.pkl\", \"rb\") as f:\n",
    "    metadata_faiss = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "75f27273-8186-4ed1-a7b9-f3c4dfa41394",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_top_k_by_type(query_embedding, k=5, target_type=\"text\"):\n",
    "    \"\"\"\n",
    "    Retrieves the top-k nearest neighbors from the FAISS index filtered by type.\n",
    "\n",
    "    Parameters:\n",
    "        query_embedding (torch.Tensor): The query embedding.\n",
    "        k (int): Number of top results to retrieve.\n",
    "        target_type (str): The type of results to retrieve (\"text\" or \"image\").\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list of metadata for the top-k results of the specified type.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert the query embedding to numpy and normalize\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype(np.float32)\n",
    "    \n",
    "    # Step 2: Perform the search in the FAISS index\n",
    "    distances, indices = index.search(query_embedding, index.ntotal)  # Search all entries\n",
    "    \n",
    "    # Step 3: Filter results by the desired type\n",
    "    filtered_results = []\n",
    "    for idx, i in enumerate(indices[0]):\n",
    "        result = {**metadata[i], \"distance\": distances[0][idx]}\n",
    "        if result[\"type\"] == target_type:\n",
    "            filtered_results.append(result)\n",
    "        if len(filtered_results) >= k:  # Stop after retrieving enough results\n",
    "            break\n",
    "    \n",
    "    return filtered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "33afb15c-76df-4d9d-8ece-c9e5135ecdbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def generate_query_embedding(text_query=None, image_query_path=None, text_weight=0.5, image_weight=0.5):\n",
    "    \"\"\"\n",
    "    Generates a query embedding based on the provided text and/or image query.\n",
    "\n",
    "    Parameters:\n",
    "        text_query (str): Text query for the search (optional).\n",
    "        image_query_path (str): Path to the query image (optional).\n",
    "        text_weight (float): Weight for the text embedding (default 0.5).\n",
    "        image_weight (float): Weight for the image embedding (default 0.5).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized query embedding for FAISS search.\n",
    "    \"\"\"\n",
    "    text_embedding = None\n",
    "    image_embedding = None\n",
    "\n",
    "    # Generate text embedding if text query is provided\n",
    "    if text_query:\n",
    "        text_inputs = clip_processor(text=[text_query], return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            text_embedding = clip_model.get_text_features(**text_inputs)\n",
    "        text_embedding = text_embedding.cpu().numpy()  # Convert to NumPy array\n",
    "\n",
    "    # Generate image embedding if image query is provided\n",
    "    if image_query_path:\n",
    "        response = requests.get(image_query_path, timeout=10)\n",
    "        query_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        image_inputs = clip_processor(images=[query_image], return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            image_embedding = clip_model.get_image_features(**image_inputs)\n",
    "        image_embedding = image_embedding.cpu().numpy()  # Convert to NumPy array\n",
    "\n",
    "    # Handle the three cases: only text, only image, or both\n",
    "    if text_embedding is not None and image_embedding is not None:\n",
    "        # Combine text and image embeddings\n",
    "        combined_embedding = text_weight * text_embedding + image_weight * image_embedding\n",
    "        query_embedding = combined_embedding\n",
    "    elif text_embedding is not None:\n",
    "        query_embedding = text_embedding  # Only text query\n",
    "    elif image_embedding is not None:\n",
    "        query_embedding = image_embedding  # Only image query\n",
    "    else:\n",
    "        raise ValueError(\"At least one of text_query or image_query_path must be provided.\")\n",
    "\n",
    "    # Normalize the query embedding (L2 normalization)\n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    if norm > 0:\n",
    "        normalized_query_embedding = query_embedding / norm\n",
    "    else:\n",
    "        normalized_query_embedding = query_embedding  # Handle zero vector case\n",
    "\n",
    "    return normalized_query_embedding.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b7b590cb-2cfc-4226-b79e-cbeff0c5ea86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'text', 'text_id': 227, 'content': 'Rayne Longboards Demonseed Longboard Complete | Sports & Outdoors | Outdoor Recreation | Skates, Skateboards & Scooters | Skateboarding | Standard Skateboards & Longboards | Longboards | $249.95', 'distance': 0.5939774}, {'type': 'text', 'text_id': 903, 'content': 'Yocaher New VW Vibe Beach Series Longboard Complete Cruiser and Decks Available for All Shapes (Complete-Oldschool-Blue) | Sports & Outdoors | Outdoor Recreation | Skates, Skateboards & Scooters | Skateboarding | Standard Skateboards & Longboards | Longboards | $69.99', 'distance': 0.5876462}, {'type': 'text', 'text_id': 0, 'content': 'DB Longboards CoreFlex Crossbow 41\" Bamboo Fiberglass Longboard Complete | Sports & Outdoors | Outdoor Recreation | Skates, Skateboards & Scooters | Skateboarding | Standard Skateboards & Longboards | Longboards | $237.68', 'distance': 0.58075}]\n",
      "[{'type': 'image', 'image_id': 0, 'path': 'https://images-na.ssl-images-amazon.com/images/I/51j3fPQTQkL.jpg|https://images-na.ssl-images-amazon.com/images/I/31hKM3cSoSL.jpg|https://images-na.ssl-images-amazon.com/images/I/51WlHdwghfL.jpg|https://images-na.ssl-images-amazon.com/images/I/51FsyLRBzwL.jpg|https://images-na.ssl-images-amazon.com/images/G/01/x-locale/common/transparent-pixel.jpg', 'distance': 0.84743756}, {'type': 'image', 'image_id': 227, 'path': 'https://images-na.ssl-images-amazon.com/images/I/41j14nHDzgL.jpg|https://images-na.ssl-images-amazon.com/images/I/51-y4qHwvSL.jpg|https://images-na.ssl-images-amazon.com/images/I/21ZHNx6MXyL.jpg|https://images-na.ssl-images-amazon.com/images/I/21zyJmzKL3L.jpg|https://images-na.ssl-images-amazon.com/images/I/21k-TxZ9nfL.jpg|https://images-na.ssl-images-amazon.com/images/G/01/x-locale/common/transparent-pixel.jpg', 'distance': 0.749098}, {'type': 'image', 'image_id': 589, 'path': 'https://images-na.ssl-images-amazon.com/images/I/51yBQaS9Z2L.jpg|https://images-na.ssl-images-amazon.com/images/I/41hRbd%2Bgb6L.jpg|https://images-na.ssl-images-amazon.com/images/I/41582vi%2B1fL.jpg|https://images-na.ssl-images-amazon.com/images/I/41-PyKQO3pL.jpg|https://images-na.ssl-images-amazon.com/images/I/41U3DmXl%2B-L.jpg|https://images-na.ssl-images-amazon.com/images/I/41JBRwQf42L.jpg|https://images-na.ssl-images-amazon.com/images/I/31XY4lVHtXL.jpg|https://images-na.ssl-images-amazon.com/images/G/01/x-locale/common/transparent-pixel.jpg', 'distance': 0.7279495}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "\n",
    "# # Case 1: Text query only\n",
    "# query_text = \"Can you provide a detailed introduction to DB Longboards and their product range?\"\n",
    "# query_embedding = generate_query_embedding(text_query=query_text)\n",
    "\n",
    "# # Case 2: Image query only\n",
    "# query_image_path = image_urls[0]  # Replace with your image path\n",
    "# query_embedding = generate_query_embedding(image_query_path=query_image_path)\n",
    "\n",
    "# Case 3: Both text and image query\n",
    "query_text = \"Can you provide a detailed introduction to DB Longboards and their product range?\"\n",
    "query_image_path = image_urls[0]\n",
    "query_embedding = generate_query_embedding(text_query=query_text, image_query_path=query_image_path)\n",
    "\n",
    "# Retrieve top-5 relevant items\n",
    "top_k_results_text = retrieve_top_k_by_type(query_embedding, k=3, target_type=\"text\")\n",
    "top_k_results_image = retrieve_top_k_by_type(query_embedding, k=3, target_type=\"image\")\n",
    "\n",
    "print(top_k_results_text)\n",
    "print(top_k_results_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8a9cec4a-bb16-4de4-afb0-1d081cfa41e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rag_with_llm(query_text, query_image_path=None, k=3):\n",
    "    \"\"\"\n",
    "    Retrieves relevant text and image results and generates a response using an LLM.\n",
    "\n",
    "    Parameters:\n",
    "        query_text (str): The text query from the user.\n",
    "        query_image_path (str): Path to an optional image query.\n",
    "        k (int): Number of top results to retrieve per type.\n",
    "\n",
    "    Returns:\n",
    "        str: The LLM's response (text, image references, or both).\n",
    "    \"\"\"\n",
    "    # Step 1: Generate query embedding\n",
    "    query_embedding = generate_query_embedding(\n",
    "        text_query=query_text, image_query_path=query_image_path\n",
    "    )\n",
    "    \n",
    "    # Step 2: Retrieve top-k text and image results\n",
    "    text_results = retrieve_top_k_by_type(query_embedding, k=k, target_type=\"text\")\n",
    "    image_results = retrieve_top_k_by_type(query_embedding, k=k, target_type=\"image\")\n",
    "    \n",
    "    # Step 3: Build the LLM prompt\n",
    "    prompt = \"You are a helpful assistant. Respond based on the query and retrieved results.\\n\\n\"\n",
    "    prompt += f\"User Query: {query_text}\\n\\n\"\n",
    "    \n",
    "    if query_image_path:\n",
    "        prompt += f\"User also uploaded an image: {query_image_path}\\n\\n\"\n",
    "    \n",
    "    prompt += \"Retrieved Text Results:\\n\"\n",
    "    for i, result in enumerate(text_results):\n",
    "        prompt += f\"{i+1}. {result['content']} (Score: {result['distance']:.2f})\\n\"\n",
    "    \n",
    "    prompt += \"\\nRetrieved Image Results:\\n\"\n",
    "    for i, result in enumerate(image_results):\n",
    "        prompt += f\"{i+1}. Image URL: {result['path']} (Score: {result['distance']:.2f})\\n\"\n",
    "    \n",
    "    prompt += \"\\nPlease respond appropriately with text, images, or both, based on the query and retrieved results.\\n\"\n",
    "\n",
    "    # Step 4: Generate LLM response\n",
    "    response = llm_generate(prompt)  # Replace with your LLM's generate method\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4a2142d4-0b53-4770-a224-1bf959b1cb90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def llm_generate(prompt):\n",
    "    \"\"\"\n",
    "    Generates a response from an LLM (e.g., OpenAI GPT) given a prompt.\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The input prompt for the LLM.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response.\n",
    "    \"\"\"\n",
    "    client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # Replace with your model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7789cee8-3513-4600-8ac5-1628d66f9d08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      "It seems that the retrieved results do not provide specific information about DB Longboards. However, I can offer a general overview of longboards and what makes DB Longboards popular.\n",
      "\n",
      "### About DB Longboards\n",
      "DB Longboards is known for crafting high-quality longboards that cater to a variety of riding styles, including cruising, carving, and downhill racing. Their boards often feature unique designs and are made from durable materials to ensure a smooth ride and longevity. DB Longboards typically focus on providing a balance of performance and aesthetics, making them a favorite among both casual riders and serious enthusiasts.\n",
      "\n",
      "### Related Products\n",
      "While I couldn't find specific longboards from DB, here are some general products related to games and accessories that were retrieved:\n",
      "\n",
      "1. **Nuts** - $9.95\n",
      "   ![Nuts](https://images-na.ssl-images-amazon.com/images/I/41ZVAsZsdNL.jpg)\n",
      "\n",
      "2. **What's In the Box** - $19.25\n",
      "   ![What's In the Box](https://images-na.ssl-images-amazon.com/images/I/51u7aZt4-hL.jpg)\n",
      "\n",
      "3. **Spot It!** (Color/Packaging May Vary) - $9.99\n",
      "   ![Spot It!](https://images-na.ssl-images-amazon.com/images/I/513Ylp54iuL.jpg)\n",
      "\n",
      "4. **Diamonsters** - $12.40\n",
      "   ![Diamonsters](https://images-na.ssl-images-amazon.com/images/I/31z0GoLEziL.jpg)\n",
      "\n",
      "5. **The Dead of Night** - $47.99\n",
      "   ![The Dead of Night](https://images-na.ssl-images-amazon.com/images/I/31I5GQV4rFL.jpg)\n",
      "\n",
      "If you're looking for specific longboards from DB or similar brands, I recommend checking out dedicated skateboarding or longboarding retailers online.\n"
     ]
    }
   ],
   "source": [
    "# Query Example\n",
    "query_text = \"Tell me about DB Longboards and show me related products.\"\n",
    "query_image_path = None  # Example: Add a path to an image if applicable\n",
    "\n",
    "# Run the RAG pipeline\n",
    "response = rag_with_llm(query_text, query_image_path=query_image_path, k=5)\n",
    "\n",
    "# Output the response\n",
    "print(\"LLM Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113a8658-0ed7-4171-886e-2dca7e81f8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd733cb0-794d-486b-a9b1-cac305e636d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd55136-692a-4392-bca6-1e43f8092a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cdef6e4-2cb1-44fb-aed5-ed3328744ccd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace with your token\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28c37898-cc00-4aeb-bf82-39a0642009a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/env_jason/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards: 100%|██████████| 4/4 [06:21<00:00, 95.32s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a84ec78c-2be1-4653-9362-1857d1d16eab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 14.57 GiB of which 166.75 MiB is free. Process 76593 has 756.00 MiB memory in use. Including non-PyTorch memory, this process has 13.66 GiB memory in use. Of the allocated memory 12.93 GiB is allocated by PyTorch, and 627.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prepare_prompt_with_texts_and_images(query_text, top_k_results)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Generate the LLM response\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/envs/env_jason/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:272\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/env_jason/lib/python3.8/site-packages/transformers/pipelines/base.py:1302\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         )\n\u001b[1;32m   1300\u001b[0m     )\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/env_jason/lib/python3.8/site-packages/transformers/pipelines/base.py:1309\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1308\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1309\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1310\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/conda/envs/env_jason/lib/python3.8/site-packages/transformers/pipelines/base.py:1209\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1208\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1209\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/env_jason/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:370\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    368\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 370\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/env_jason/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/env_jason/lib/python3.8/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/env_jason/lib/python3.8/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/env_jason/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/env_jason/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/env_jason/lib/python3.8/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/env_jason/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:1210\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[0;32m-> 1210\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1212\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/env_jason/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/env_jason/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/env_jason/lib/python3.8/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/conda/envs/env_jason/lib/python3.8/site-packages/accelerate/hooks.py:355\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    348\u001b[0m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    349\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    350\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdata_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map\n\u001b[1;32m    351\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map[value\u001b[38;5;241m.\u001b[39mdata_ptr()]\n\u001b[1;32m    352\u001b[0m         ):\n\u001b[1;32m    353\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove\u001b[38;5;241m.\u001b[39madd((value\u001b[38;5;241m.\u001b[39mdata_ptr(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device))\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(\n\u001b[1;32m    365\u001b[0m     kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys\n\u001b[1;32m    366\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/env_jason/lib/python3.8/site-packages/accelerate/utils/modeling.py:329\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    327\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 329\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 14.57 GiB of which 166.75 MiB is free. Process 76593 has 756.00 MiB memory in use. Including non-PyTorch memory, this process has 13.66 GiB memory in use. Of the allocated memory 12.93 GiB is allocated by PyTorch, and 627.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def prepare_prompt_with_texts_and_images(user_query, retrieved_items):\n",
    "    \"\"\"\n",
    "    Combines retrieved text and images into a prompt for the LLM.\n",
    "    \"\"\"\n",
    "    text_chunks = [item[\"chunk\"] for item in retrieved_items if item[\"type\"] == \"text\"]\n",
    "    image_paths = [item[\"path\"] for item in retrieved_items if item[\"type\"] == \"image\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    User Query: {user_query}\n",
    "\n",
    "    Context:\n",
    "    - Relevant Texts: {\" \".join(text_chunks)}\n",
    "    - Relevant Images: {\", \".join(image_paths)}\n",
    "\n",
    "    Provide a detailed response based on the context.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Generate the prompt\n",
    "prompt = prepare_prompt_with_texts_and_images(query_text, top_k_results)\n",
    "\n",
    "# Generate the LLM response\n",
    "response = pipeline(prompt, max_new_tokens=50,)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0a31e-6737-4b28-8343-471cfb74c9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb5f9d2-7cdd-4d30-b711-fad4e626b0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9dda33-4306-4a85-a04d-c024ad002d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb9bdb4-9a48-4767-917d-d41e2458be4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c08f661-5838-49d0-97d1-24802c2b7236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "\n",
    "text_documents = [Document(page_content=text) for text in text_descriptions]\n",
    "image_documents = [Document(page_content=path) for path in image_paths]\n",
    "\n",
    "documents = text_documents + image_documents\n",
    "embeddings = np.vstack([text_embeddings, image_embeddings]).tolist()  # Stack text and image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad92f4-395c-46e0-b0f9-fcf0f9ec3201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize Chroma client\n",
    "persist_dir = \"chroma_multimodal_db\"\n",
    "client = chromadb.Client(Settings(persist_directory=persist_dir))\n",
    "\n",
    "# Create or load a collection\n",
    "collection = client.get_or_create_collection(\"multimodal_collection\")\n",
    "\n",
    "# Add documents with embeddings\n",
    "documents = [doc.page_content for doc in text_documents + image_documents]  # Document content\n",
    "metadatas = [{\"type\": \"text\"}] * len(text_documents) + [{\"type\": \"image\"}] * len(image_documents)  # Metadata\n",
    "ids = [f\"id_{i}\" for i in range(len(documents))]  # Unique IDs\n",
    "\n",
    "# Add data to the collection\n",
    "collection.add(\n",
    "    embeddings=embeddings,  # Precomputed embeddings\n",
    "    documents=documents,  # Corresponding text or image paths\n",
    "    metadatas=metadatas,  # Metadata (optional)\n",
    "    ids=ids  # Unique IDs\n",
    ")\n",
    "\n",
    "print(\"Data successfully added to the collection!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d161dd26-d7f4-42be-9457-4a0ba27b6823",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_text_embeddings_in_batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate a query embedding\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mget_text_embeddings_in_batches\u001b[49m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan you show me a picture of the DB Longboards?\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Convert the PyTorch tensor to a Python list\u001b[39;00m\n\u001b[1;32m      5\u001b[0m query_embedding_list \u001b[38;5;241m=\u001b[39m query_embedding\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_text_embeddings_in_batches' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate a query embedding\n",
    "query_embedding = get_text_embeddings_in_batches([\"Can you show me a picture of the DB Longboards?\"])[0]\n",
    "\n",
    "# Convert the PyTorch tensor to a Python list\n",
    "query_embedding_list = query_embedding.cpu().tolist()\n",
    "\n",
    "# Query the collection\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding_list],\n",
    "    n_results=5  # Number of top results\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "for document, metadata in zip(results[\"documents\"], results[\"metadatas\"]):\n",
    "    print(f\"Document: {document}, Metadata: {metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dfd28e-849a-4a7e-aabe-5d232ec89dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the Chroma vector store\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"multimodal_collection\",\n",
    "    persist_directory=persist_dir,\n",
    "    embedding_function=None  # Already embedded documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5774892-7e60-4d21-ac9d-f956c31944bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Collection Name (Chroma client): {collection.name}\")\n",
    "print(f\"Persist Directory (Chroma client): {persist_directory}\")\n",
    "print(f\"Collection Name (LangChain): {vectorstore._collection.name}\")\n",
    "print(f\"Persist Directory (LangChain): {persist_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e32ce46-c5c5-4c7d-9f76-aaa3dc3ca88d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection_v = vectorstore._collection\n",
    "\n",
    "collection_item_count = len(collection_v.get(include=[\"documents\"])[\"documents\"])\n",
    "print(f\"Number of items in the Chroma collection: {collection_item_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561fe1c4-4fbc-4932-9a0c-f4a2b9e07471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the number of items in the LangChain vector store\n",
    "vectorstore_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "vectorstore_results = vectorstore_retriever.get_relevant_documents(\"test query\")\n",
    "print(f\"Number of items in the LangChain vector store: {len(vectorstore_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdc8495-28ee-46fa-9dc9-95c21a413cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the LLM\n",
    "model_name = \"meta-llama-3b\"  # Replace with your LLM model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Create a pipeline\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "# Wrap the pipeline for LangChain\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0570baf-5d80-468d-93b6-93cead9d333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template\n",
    "template = \"\"\"\n",
    "You are a helpful assistant capable of answering questions based on provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Query:\n",
    "{question}\n",
    "\n",
    "Your Response:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Create a conversational retrieval chain\n",
    "retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True,  # Include retrieved documents in the output\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88ed75-de1c-47d7-9a33-7b8ab25c996e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328cb153-ff69-434b-a0ff-32b64d53ac76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class CLIPEmbeddingFunction(Embeddings):\n",
    "    def __init__(self, clip_model, clip_processor, device, batch_size=16):\n",
    "        self.clip_model = clip_model\n",
    "        self.clip_processor = clip_processor\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size  # Set batch size for processing\n",
    "\n",
    "    def embed_documents(self, texts_or_images):\n",
    "        embeddings = []\n",
    "\n",
    "        for i in range(0, len(texts_or_images), self.batch_size):\n",
    "            batch = texts_or_images[i:i + self.batch_size]\n",
    "\n",
    "            if all(isinstance(x, str) and x.endswith((\".jpg\", \".png\")) for x in batch):  # Image paths\n",
    "                images = [Image.open(path).convert(\"RGB\") for path in batch]\n",
    "                inputs = self.clip_processor(images=images, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    batch_embeddings = self.clip_model.get_image_features(**inputs).cpu().tolist()\n",
    "            elif all(isinstance(x, str) for x in batch):  # Text\n",
    "                inputs = self.clip_processor(text=batch, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    batch_embeddings = self.clip_model.get_text_features(**inputs).cpu().tolist()\n",
    "            else:\n",
    "                raise ValueError(\"Inputs must be a list of text strings or image paths.\")\n",
    "\n",
    "            embeddings.extend(batch_embeddings)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        inputs = self.clip_processor(text=[query], return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.clip_model.get_text_features(**inputs).cpu().tolist()\n",
    "        return embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc78a39-07d0-4291-bebc-a973566d068f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the custom embedding function\n",
    "clip_embedding_function = CLIPEmbeddingFunction(clip_model, clip_processor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5227aa6-8efa-453e-a1ed-2c4a3d3836a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_documents = [Document(page_content=text) for text in text_descriptions]\n",
    "image_documents = [Document(page_content=path) for path in image_paths]\n",
    "\n",
    "documents = text_documents + image_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed1aff3-f90a-45dd-b9f0-7af242a60a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "persist_dir = \"chroma_embeddings_db\"\n",
    "\n",
    "# Create the vector store with dynamic embeddings\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=clip_embedding_function,\n",
    "    persist_directory=persist_dir\n",
    ")\n",
    "\n",
    "# Persist the vectorstore\n",
    "vectorstore.persist()\n",
    "print(\"Vectorstore created and persisted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbdedb7-eb5f-412b-91cf-38c9074800ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a3951-d899-4753-9742-6f96c068b489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import os\n",
    "\n",
    "text_documents = [Document(page_content=text) for text in text_descriptions]\n",
    "image_documents = [Document(page_content=path) for path in image_paths]\n",
    "\n",
    "# Combine embeddings and documents\n",
    "documents = text_documents + image_documents\n",
    "embeddings = np.vstack([text_embeddings, image_embeddings]).tolist()  # Stack text and image embeddings\n",
    "\n",
    "# Initialize Chroma database\n",
    "persist_dir = \"chroma_embeddings_db\"\n",
    "client = chromadb.Client(Settings(persist_directory=persist_dir, chroma_db_impl=\"duckdb+parquet\"))\n",
    "\n",
    "# Create a collection in Chroma\n",
    "collection = client.get_or_create_collection(\"my_embeddings_collection\")\n",
    "\n",
    "# Insert precomputed embeddings into the collection\n",
    "documents = [doc.page_content for doc in text_documents + image_documents]\n",
    "metadata = [{\"type\": \"text\"}] * len(text_documents) + [{\"type\": \"image\"}] * len(image_documents)\n",
    "\n",
    "collection.add(\n",
    "    embeddings=embeddings,  # Precomputed embeddings\n",
    "    documents=documents,  # Corresponding documents\n",
    "    metadatas=metadata,  # Metadata for each document\n",
    "    ids=[f\"id_{i}\" for i in range(len(documents))]  # Unique IDs for each document\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b2d96-8dd0-43bd-83b6-1d274cbf55e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbfd062-0f3a-4d96-9488-8696fa0ea76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f829b-73e3-4b0c-986d-884ccdc5c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bdae36-1d75-4088-8918-886e3522f168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d334482-1e18-4346-9cf5-6770780a63eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# vectorstore = Chroma.from_documents(documents=chunks,\n",
    "#                                     embedding=embedding)\n",
    "\n",
    "vectorstore_openai_embed = Chroma.from_documents(documents=chunks,\n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "retriever_openai_embed = vectorstore_openai_embed.as_retriever(search_kwargs={\"k\": 10})"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "env_jason",
   "name": ".m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m125"
  },
  "kernelspec": {
   "display_name": "env_jason (Local)",
   "language": "python",
   "name": "env_jason"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
